{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Purpose: Collect News Article titles about a currency pair and get its associated price movement (Up or Down) **"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "****"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4 requests"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install investpy"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get the titles of relevant news articles from www.investing.com**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "\n",
        "def get_forex_news_five_years(currency_pair):\n",
        "\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "    articles = []\n",
        "    \n",
        "    page_count = 0\n",
        "    stop_flag = False\n",
        "    article_titles = []\n",
        "    article_dates = []\n",
        "    while not stop_flag and page_count <= 10:\n",
        "        if page_count == 0:\n",
        "            url = 'https://www.investing.com/currencies/'+currency_pair+'-news'\n",
        "        else:\n",
        "            url = 'https://www.investing.com/currencies/'+currency_pair+'-news/'+str(page_count)\n",
        "        response = requests.get(url, headers=headers)\n",
        "        print('url:',url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "            news_list = soup.find('ul', attrs={'data-test': 'news-list'})\n",
        "\n",
        "            # Extract all <article> elements inside it\n",
        "            articles = news_list.find_all('article')\n",
        "            for article in articles:\n",
        "                div_tag = article.find('div', class_='block w-full sm:flex-1')\n",
        "                if div_tag:\n",
        "                    # Extract the <a> tag inside the <div>\n",
        "                    a_tag = div_tag.find('a', href=True)\n",
        "                else:\n",
        "                    print(\"coudln't find div tag of class block w-full sm:flex-1\")\n",
        "                    assert(False)\n",
        "                if a_tag:\n",
        "                    article_title = a_tag.get_text(strip=True)\n",
        "                    \n",
        "                else:\n",
        "                    print(\"coudln't find a tag with href\")\n",
        "                    assert(False)\n",
        "\n",
        "                time_tag = div_tag.find('time', datetime=True)\n",
        "\n",
        "                if time_tag:\n",
        "                    # Get the datetime attribute from the <time> tag\n",
        "                    datetime_value = time_tag['datetime'].split()[0]\n",
        "                    dt = datetime.strptime(datetime_value, '%Y-%m-%d').date()\n",
        "                    current_date = datetime.now().date()\n",
        "                    diff_in_years = current_date.year - dt.year\n",
        "                    if diff_in_years == 5:\n",
        "                        stop_flag = True\n",
        "                    else:\n",
        "                        article_titles.append(article_title)\n",
        "                        article_dates.append(datetime_value)\n",
        "\n",
        "\n",
        "                else:\n",
        "                    print(\"No <time> tag with datetime attribute found.\")\n",
        "                    assert(False)\n",
        "\n",
        "        else:\n",
        "            print(\"couldn't get page info:\",url)\n",
        "            assert(False)\n",
        "\n",
        "        page_count += 1\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "    'Title': article_titles,\n",
        "    'Date': article_dates\n",
        "    })\n",
        "    c1 = currency_pair.split('-')[0]\n",
        "    c2 = currency_pair.split('-')[1]\n",
        "    str_ = c1+'_'+c2\n",
        "    df.to_csv('currency_pair_info'+str_+'.csv', index=False)\n",
        "    print(\"saved csv\")\n",
        "    print(\"done\")\n",
        "\n",
        "currency_pair = 'gbp-usd'\n",
        "get_forex_news_five_years(currency_pair)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1743907470252
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build the dataset from the obtained price trends and news article titles**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def build_dataset(price_csv, titles_csv, pred_horizon, dest_file):\n",
        "\n",
        "    df_price = pd.read_csv(price_csv)\n",
        "    df_titles = pd.read_csv(titles_csv)\n",
        "\n",
        "    df_titles['Date'] = pd.to_datetime(df_titles['Date']).dt.date  # Converts to datetime.date\n",
        "    df_price['Date'] = pd.to_datetime(df_price['Date']).dt.date\n",
        "\n",
        "    date_format = \"%Y-%m-%d\"\n",
        "    dates = df_price['Date'].tolist()\n",
        "    titles_col = []\n",
        "    label_col = []\n",
        "    for datestr in dates[:-1]:\n",
        "        today = pd.to_datetime(datestr, format=date_format).date()\n",
        "        next_day_dt = today + datetime.timedelta(days=1)\n",
        "        next_day = next_day_dt.strftime(date_format)\n",
        "        df1 = df_titles[(today - df_titles['Date'] >= datetime.timedelta(days=0)) & (today - df_titles['Date'] < datetime.timedelta(days=pred_horizon))]\n",
        "        relevant_titles = df_titles[(today - df_titles['Date'] >= datetime.timedelta(days=0)) & (today - df_titles['Date'] < datetime.timedelta(days=pred_horizon))]['Title'].tolist()\n",
        "        if len(relevant_titles) == 0:\n",
        "            print('empty relevant titles detected')\n",
        "            print('date:',today)\n",
        "        today_price = df_price[df_price['Date'] == today]['Close'].tolist()[0]\n",
        "        try:\n",
        "            tomorrow_price = df_price[df_price['Date'] == next_day_dt]['Close'].tolist()[0]\n",
        "        except:\n",
        "            # case when next day is a Saturday\n",
        "            next_day_dt = today + datetime.timedelta(days=3)\n",
        "            next_day = next_day_dt.strftime(date_format)\n",
        "            tomorrow_price = df_price[df_price['Date'] == next_day_dt]['Close'].tolist()[0]\n",
        "\n",
        "        label = int(tomorrow_price > today_price)\n",
        "\n",
        "        titles = \"\\n\".join(relevant_titles)\n",
        "\n",
        "        titles_col.append(titles)\n",
        "        label_col.append(label)\n",
        "\n",
        "    \n",
        "    df = pd.DataFrame({\n",
        "    'Titles': titles_col,\n",
        "    'Label': label_col\n",
        "    })\n",
        "    df = df.dropna(subset=['Titles'])\n",
        "    df.to_csv(dest_file, index=False)\n",
        "\n",
        "price_csv = 'gbp_usd.csv'\n",
        "titles_csv = 'currency_pair_infogbp_usd.csv' \n",
        "pred_horizon = 1\n",
        "dest_file = \"gbp_usd_dataset_ph_1.csv\"\n",
        "\n",
        "build_dataset(price_csv, titles_csv, pred_horizon, dest_file)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "empty relevant titles detected\ndate: 2021-04-02\n"
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m pred_horizon \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     54\u001b[0m dest_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgbp_usd_dataset_ph_1.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 56\u001b[0m \u001b[43mbuild_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprice_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitles_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_horizon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest_file\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[1], line 26\u001b[0m, in \u001b[0;36mbuild_dataset\u001b[0;34m(price_csv, titles_csv, pred_horizon, dest_file)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mempty relevant titles detected\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate:\u001b[39m\u001b[38;5;124m'\u001b[39m,today)\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     27\u001b[0m today_price \u001b[38;5;241m=\u001b[39m df_price[df_price[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m today][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1744066358569
        },
        "jupyter": {
          "outputs_hidden": false
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}