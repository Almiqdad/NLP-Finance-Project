{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Purpose: Collect News Article titles about a currency pair and get its associated price movement (Up or Down) **"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "****"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4 requests\n",
        "# I made these changes bitch"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: beautifulsoup4 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (4.12.3)\nRequirement already satisfied: requests in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (2.32.3)\nRequirement already satisfied: soupsieve>1.2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from beautifulsoup4) (2.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests) (1.26.19)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests) (2024.8.30)\n\u001b[33mWARNING: Error parsing dependencies of numpy: [Errno 2] No such file or directory: '/anaconda/envs/azureml_py38/lib/python3.10/site-packages/numpy-1.23.5.dist-info/METADATA'\u001b[0m\u001b[33m\n\u001b[0m"
        }
      ],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Given a currency pair, get relevant news articles from the previous day**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup as bs\n",
        "import pandas as pd\n",
        "from urllib.request import Request, urlopen\n",
        "from tqdm import tqdm\n",
        "\n",
        "class fx_news_scraper():\n",
        "    def __init__(self):\n",
        "        self.number_of_pages = 29\n",
        "        self.news_list = []\n",
        "        self.target_url = 'https://www.investing.com/news/forex-news/'\n",
        "        self.output_csv = 'app/data/fx-news-archive_test.csv'\n",
        "        self.news_dataset = pd.DataFrame()\n",
        "\n",
        "    def scraper(self):\n",
        "        for page_no in tqdm(range(1, self.number_of_pages+1)):\n",
        "            url = self.target_url + str(page_no)\n",
        "            req = Request(url , headers={'User-Agent': 'Mozilla/5.0'})\n",
        "            \n",
        "            webpage = urlopen(req).read()\n",
        "            \n",
        "            soup = bs(webpage, \"html.parser\")\n",
        "            \n",
        "            news_container_div = soup.find('div', class_= 'news-analysis-v2_articles-container__3fFL8 mdMax:px-3 mb-12')\n",
        "\n",
        "            try:\n",
        "                for news_div in news_container_div.find_all('div', class_= 'news-analysis-v2_content__z0iLP w-full text-xs sm:flex-1'):\n",
        "                    news_title_a = news_div.find('a', class_ = 'text-inv-blue-500 hover:text-inv-blue-500 hover:underline focus:text-inv-blue-500 focus:underline whitespace-normal text-sm font-bold leading-5 !text-[#181C21] sm:text-base sm:leading-6 lg:text-lg lg:leading-7')\n",
        "                    news_body    = news_div.find('p')\n",
        "                    news_date    = news_div.find('time', class_ = 'mx-1 shrink-0 text-xs leading-4').get('datetime')\n",
        "                    \n",
        "                    news_href    = news_title_a.get('href')\n",
        "                    \n",
        "                    self.news_list.append([page_no, news_date, news_title_a.text , news_body.text, news_href])\n",
        "\n",
        "                    \n",
        "            except ValueError as vEr:\n",
        "                print(vEr)\n",
        "\n",
        "            \n",
        "            self.news_dataset = pd.DataFrame(self.news_list, columns = ['page', 'date','title','body', 'url'] )\n",
        "            self.news_dataset.to_csv(self.output_csv)\n",
        "            \n",
        "            page_no += 1\n",
        "\n",
        "        print ('scraping completed')\n",
        "        print (f'output --> {self.output_csv}')"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1743783596665
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_scraper = fx_news_scraper()\n",
        "news_scraper.scraper()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\r  0%|          | 0/29 [00:00<?, ?it/s]\r  0%|          | 0/29 [00:00<?, ?it/s]\n"
        },
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "HTTP Error 403: Forbidden",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m news_scraper \u001b[38;5;241m=\u001b[39m fx_news_scraper()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mnews_scraper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscraper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[7], line 19\u001b[0m, in \u001b[0;36mfx_news_scraper.scraper\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     16\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_url \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(page_no)\n\u001b[1;32m     17\u001b[0m req \u001b[38;5;241m=\u001b[39m Request(url , headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMozilla/5.0\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m---> 19\u001b[0m webpage \u001b[38;5;241m=\u001b[39m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     21\u001b[0m soup \u001b[38;5;241m=\u001b[39m bs(webpage, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m news_container_div \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnews-analysis-v2_articles-container__3fFL8 mdMax:px-3 mb-12\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/urllib/request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[1;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/urllib/request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/urllib/request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[1;32m    562\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/urllib/request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"
          ]
        }
      ],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1743783663615
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install investpy"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting investpy\n  Downloading investpy-1.0.8.tar.gz (4.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\n\u001b[?25hCollecting Unidecode>=1.1.1 (from investpy)\n  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: setuptools>=41.2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from investpy) (68.0.0)\nRequirement already satisfied: numpy>=1.17.2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from investpy) (1.23.5)\nRequirement already satisfied: pandas>=0.25.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from investpy) (1.3.5)\nCollecting lxml>=4.4.1 (from investpy)\n  Downloading lxml-5.3.2-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.6 kB)\nRequirement already satisfied: requests>=2.22.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from investpy) (2.32.3)\nRequirement already satisfied: pytz>=2019.3 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from investpy) (2022.5)\n\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/anaconda/envs/azureml_py38/lib/python3.10/site-packages/numpy-1.23.5.dist-info/METADATA'\n\u001b[0m\u001b[31m\n\u001b[0m"
        }
      ],
      "execution_count": 13,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yfinance"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting yfinance\n  Using cached yfinance-0.2.55-py2.py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: pandas>=1.3.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from yfinance) (1.3.5)\nRequirement already satisfied: numpy>=1.16.5 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from yfinance) (1.23.5)\nRequirement already satisfied: requests>=2.31 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from yfinance) (2.32.3)\nCollecting multitasking>=0.0.7 (from yfinance)\n  Using cached multitasking-0.0.11-py3-none-any.whl.metadata (5.5 kB)\nRequirement already satisfied: platformdirs>=2.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from yfinance) (4.2.2)\nRequirement already satisfied: pytz>=2022.5 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from yfinance) (2022.5)\nCollecting frozendict>=2.3.4 (from yfinance)\n  Using cached frozendict-2.4.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)\nCollecting peewee>=3.16.2 (from yfinance)\n  Using cached peewee-3.17.9.tar.gz (3.0 MB)\n  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\n\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.11.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from yfinance) (4.12.3)\nRequirement already satisfied: soupsieve>1.2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.5)\n\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/anaconda/envs/azureml_py38/lib/python3.10/site-packages/numpy-1.23.5.dist-info/METADATA'\n\u001b[0m\u001b[31m\n\u001b[0m"
        }
      ],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "\n",
        "def get_forex_news_five_years(currency_pair):\n",
        "\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "    articles = []\n",
        "    \n",
        "    page_count = 0\n",
        "    stop_flag = False\n",
        "    article_titles = []\n",
        "    article_dates = []\n",
        "    while not stop_flag and page_count <= 10:\n",
        "        if page_count == 0:\n",
        "            url = 'https://www.investing.com/currencies/'+currency_pair+'-news'\n",
        "        else:\n",
        "            url = 'https://www.investing.com/currencies/'+currency_pair+'-news/'+str(page_count)\n",
        "        response = requests.get(url, headers=headers)\n",
        "        print('url:',url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "            news_list = soup.find('ul', attrs={'data-test': 'news-list'})\n",
        "\n",
        "            # Extract all <article> elements inside it\n",
        "            articles = news_list.find_all('article')\n",
        "            for article in articles:\n",
        "                div_tag = article.find('div', class_='block w-full sm:flex-1')\n",
        "                if div_tag:\n",
        "                    # Extract the <a> tag inside the <div>\n",
        "                    a_tag = div_tag.find('a', href=True)\n",
        "                else:\n",
        "                    print(\"coudln't find div tag of class block w-full sm:flex-1\")\n",
        "                    assert(False)\n",
        "                if a_tag:\n",
        "                    article_title = a_tag.get_text(strip=True)\n",
        "                    \n",
        "                else:\n",
        "                    print(\"coudln't find a tag with href\")\n",
        "                    assert(False)\n",
        "\n",
        "                time_tag = div_tag.find('time', datetime=True)\n",
        "\n",
        "                if time_tag:\n",
        "                    # Get the datetime attribute from the <time> tag\n",
        "                    datetime_value = time_tag['datetime'].split()[0]\n",
        "                    dt = datetime.strptime(datetime_value, '%Y-%m-%d').date()\n",
        "                    current_date = datetime.now().date()\n",
        "                    diff_in_years = current_date.year - dt.year\n",
        "                    if diff_in_years == 5:\n",
        "                        stop_flag = True\n",
        "                    else:\n",
        "                        article_titles.append(article_title)\n",
        "                        article_dates.append(datetime_value)\n",
        "\n",
        "\n",
        "                else:\n",
        "                    print(\"No <time> tag with datetime attribute found.\")\n",
        "                    assert(False)\n",
        "\n",
        "        else:\n",
        "            print(\"couldn't get page info:\",url)\n",
        "            assert(False)\n",
        "\n",
        "        page_count += 1\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "    'Title': article_titles,\n",
        "    'Date': article_dates\n",
        "    })\n",
        "    c1 = currency_pair.split('-')[0]\n",
        "    c2 = currency_pair.split('-')[1]\n",
        "    str_ = c1+'_'+c2\n",
        "    df.to_csv('currency_pair_info'+str_+'.csv', index=False)\n",
        "    print(\"saved csv\")\n",
        "    print(\"done\")\n",
        "\n",
        "currency_pair = 'gbp-usd'\n",
        "get_forex_news_five_years(currency_pair)\n"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'investpy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minvestpy\u001b[39;00m\n\u001b[1;32m      6\u001b[0m currency_pair \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEURUSD=X\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Example for EUR/USD\u001b[39;00m\n\u001b[1;32m      7\u001b[0m date \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2025-04-05\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'investpy'"
          ]
        }
      ],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1743907470252
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "a = datetime.date(2011,11,24)\n",
        "b = datetime.date(2011,11,17)\n",
        "(a-b).days\n",
        "start_date = '2021-01-04'\n",
        "day = 'mon'\n",
        "date_format = \"%Y-%m-%d\"\n",
        "today = pd.to_datetime(start_date, format=date_format).date()\n",
        "today"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "datetime.date(2021, 1, 4)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1743955282552
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def build_dataset(price_csv, titles_csv, pred_horizon, dest_file):\n",
        "\n",
        "    df_price = pd.read_csv(price_csv)\n",
        "    df_titles = pd.read_csv(titles_csv)\n",
        "\n",
        "    df_titles['Date'] = pd.to_datetime(df_titles['Date']).dt.date  # Converts to datetime.date\n",
        "    df_price['Date'] = pd.to_datetime(df_price['Date']).dt.date\n",
        "\n",
        "    date_format = \"%Y-%m-%d\"\n",
        "    dates = df_price['Date'].tolist()\n",
        "    titles_col = []\n",
        "    label_col = []\n",
        "    for datestr in dates:\n",
        "        today = pd.to_datetime(datestr, format=date_format).date()\n",
        "        next_day_dt = today + datetime.timedelta(days=1)\n",
        "        next_day = next_day_dt.strftime(date_format)\n",
        "        #pd.to_datetime(df_titles['Date'], format=date_format).date()\n",
        "        #assert(False)\n",
        "        df1 = df_titles[(today - df_titles['Date'] >= datetime.timedelta(days=0)) & (today - df_titles['Date'] < datetime.timedelta(days=pred_horizon))]\n",
        "        relevant_titles = df_titles[(today - df_titles['Date'] >= datetime.timedelta(days=0)) & (today - df_titles['Date'] < datetime.timedelta(days=pred_horizon))]['Title'].tolist()\n",
        "        today_price = df_price[df_price['Date'] == today]['Close'].tolist()[0]\n",
        "        print('today price:',today_price)\n",
        "        try:\n",
        "            tomorrow_price = df_price[df_price['Date'] == next_day]['Close'].tolist()[0]\n",
        "        except:\n",
        "            # case when next day is a Saturday\n",
        "            next_day_dt = today + datetime.timedelta(days=3)\n",
        "            next_day = next_day_dt.strftime(date_format)\n",
        "            tomorrow_price = df_price[df_price['Date'] == next_day]['Close'].tolist()[0]\n",
        "\n",
        "        print('tomorrow price:',tomorrow_price)\n",
        "        label = int(tomorrow_price > today_price)\n",
        "\n",
        "        titles = \"\\n\".join(relevant_titles)\n",
        "\n",
        "        titles_col.append(titles)\n",
        "        label_col.append(label)\n",
        "\n",
        "    \n",
        "    df = pd.DataFrame({\n",
        "    'Titles': titles_col,\n",
        "    'Label': label_col\n",
        "    })\n",
        "\n",
        "    df.to_csv(dest_file, index=False)\n",
        "\n",
        "price_csv = 'gbp_usd.csv'\n",
        "titles_csv = 'currency_pair_infogbp_usd.csv' \n",
        "pred_horizon = 5\n",
        "dest_file = \"gbp_usd_dataset.csv\"\n",
        "\n",
        "build_dataset(price_csv, titles_csv, pred_horizon, dest_file)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "today price: 1.3684196472167969\n"
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[68], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m pred_horizon \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m     57\u001b[0m dest_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgbp_usd_dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 59\u001b[0m \u001b[43mbuild_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprice_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitles_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_horizon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest_file\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[68], line 27\u001b[0m, in \u001b[0;36mbuild_dataset\u001b[0;34m(price_csv, titles_csv, pred_horizon, dest_file)\u001b[0m\n\u001b[1;32m     25\u001b[0m today_price \u001b[38;5;241m=\u001b[39m df_price[df_price[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m today][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoday price:\u001b[39m\u001b[38;5;124m'\u001b[39m,today_price)\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m     tomorrow_price \u001b[38;5;241m=\u001b[39m df_price[df_price[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m next_day][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()[\u001b[38;5;241m0\u001b[39m]\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "execution_count": 68,
      "metadata": {
        "gather": {
          "logged": 1743965397309
        },
        "jupyter": {
          "outputs_hidden": false
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "df_dataset = pd.read_csv('gbp_usd_dataset.csv')\n",
        "L = df_dataset['Label'].tolist()\n",
        "print(L.count(1))\n",
        "print(L.count(0))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "177\n931\n"
        }
      ],
      "execution_count": 56,
      "metadata": {
        "gather": {
          "logged": 1743964395024
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}